{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/quora-pairs/quora_train.csv\n/kaggle/input/quora-pairs/quora_test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## import packages\n\nimport os\nimport re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\n\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score, roc_auc_score, plot_confusion_matrix","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## set directories and parameters\n\nEMBEDDING_FILE = '../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin'\nTRAIN_DATA_FILE = '../input/quora-pairs/quora_train.csv'\nTEST_DATA_FILE = '../input/quora-pairs/quora_test.csv'\nMAX_SEQUENCE_LENGTH = 30\nMAX_NB_WORDS = 200000\nEMBEDDING_DIM = 300\nVALIDATION_SPLIT = 0.1\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## process texts in datasets\nprint('Processing text dataset')\n\ndef text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)\n\ntexts_3 = [] \nlabels = []\nwith codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        texts_3.append(text_to_wordlist(values[3], values[4]))\n        labels.append(int(values[5]))\nprint('Found %s texts in train.csv' % len(texts_3))\n\ntest_texts_3 = []\ntest_ids = []\ntest_labels = []\nwith codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        test_texts_3.append(text_to_wordlist(values[3], values[4]))\n        test_ids.append(values[0])\n        test_labels.append(int(values[5]))\nprint('Found %s texts in test.csv' % len(test_texts_3))\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts_3 + test_texts_3)\n\nsequences_3 = tokenizer.texts_to_sequences(texts_3)\ntest_sequences_3 = tokenizer.texts_to_sequences(test_texts_3)\n#test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))\n\ndata_3 = pad_sequences(sequences_3, maxlen=MAX_SEQUENCE_LENGTH)\n#data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = np.array(labels)\nprint('Shape of data tensor:', data_3.shape)\nprint('Shape of label tensor:', labels.shape)\n\ntest_data_3 = pad_sequences(test_sequences_3, maxlen=MAX_SEQUENCE_LENGTH)\n#test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\ntest_ids = np.array(test_ids)\ntest_labels = np.array(test_labels)","execution_count":null,"outputs":[{"output_type":"stream","text":"Processing text dataset\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## sample train/validation data\n\n#np.random.seed(123)\nperm = np.random.permutation(len(data_3))\nidx_train = perm[:int(len(data_3)*(1-VALIDATION_SPLIT))]\nidx_val = perm[int(len(data_3)*(1-VALIDATION_SPLIT)):]\n\ndata_3_train = data_3[idx_train]\n#data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\nlabels_train = labels[idx_train]\n\ndata_3_val = data_3[idx_val]\n#data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\nlabels_val = labels[idx_val]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\n\n#X_train = np.hstack((data_1_train, data_2_train))\n#X_val = sp.sparse.hstack((data_1_val, data_2_val))\nlr = LogisticRegression()\nlr.fit(data_3_train, labels_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('predicting ...')\ny_pred = lr.predict(data_3_val)\nloss = log_loss(labels_val,y_pred)\nprint('log_loss= {}'.format(loss))\naccuracy = accuracy_score(labels_val,y_pred)\nprint('accuracy= {}'.format(accuracy))\nauc = roc_auc_score(labels_val,y_pred)\nprint('auc  = {}'.format(auc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}